{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3tRl6ZSk8Oi"
   },
   "source": [
    "This notebook use for tunning model using embeddings file and language model embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz47D5H_R0UR"
   },
   "source": [
    "### Check GPU hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8kCH-Zfj2J_",
    "outputId": "86d483f6-7c82-4be8-8713-5c77b0966d3e"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiMnInVvlEjY"
   },
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBCZs6wgdV7E",
    "outputId": "f99d1941-a359-425e-aac3-514215ccd8bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries for system and debug\n",
    "import sys\n",
    "import pdb\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Class for converting sequences to tensors\n",
    "from seq2tensor import s2t\n",
    "\n",
    "# Libraries for neural network training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional, Input, Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import Add, Flatten, subtract, multiply, concatenate\n",
    "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Import accessory modules\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjxFKiABLDob"
   },
   "source": [
    "### Set CUDA environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wmbio/WORK/gitworking/PIPR'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjvCG7HCp0Af",
    "outputId": "1dc8ea23-ae15-4c5b-ac61-ebd1a32b1b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 08:58:47.606020: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-08 08:58:47.995297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10243 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "### Setting RAM GPU for training growth \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjvCG7HCp0Af",
    "outputId": "1dc8ea23-ae15-4c5b-ac61-ebd1a32b1b9e"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Optimisation Flags - Do not remove\n",
    "# ============================================\n",
    "\n",
    "# Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation. When disabled,\n",
    "# no binary code is added to or retrieved from the cache.\n",
    "os.environ['CUDA_CACHE_DISABLE'] = '0' # orig is 0\n",
    "\n",
    "# When set to 1, forces the device driver to ignore any binary code embedded in an application \n",
    "# (see Application Compatibility) and to just-in-time compile embedded PTX code instead.\n",
    "# If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to\n",
    "# validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application \n",
    "# forward compatibility with future architectures.\n",
    "os.environ['CUDA_FORCE_PTX_JIT'] = '1'# no orig\n",
    "\n",
    "\n",
    "os.environ['HOROVOD_GPU_ALLREDUCE'] = 'NCCL'\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "\n",
    "os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
    "\n",
    "os.environ['TF_ADJUST_HUE_FUSED'] = '1'\n",
    "os.environ['TF_ADJUST_SATURATION_FUSED'] = '1'\n",
    "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "\n",
    "os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
    "os.environ['TF_AUTOTUNE_THRESHOLD'] = '2'\n",
    "os.environ['TF_DISABLE_NVTX_RANGES'] = '1'\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjTWZj-Bzebi"
   },
   "source": [
    "### Define custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_pair(seq_tensor, class_labels, pair_index):\n",
    "    for index in pair_index:\n",
    "        yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}, class_labels[index]\n",
    "\n",
    "def generator_pair_predict(seq_tensor, class_labels, pair_index):\n",
    "    for index in pair_index:\n",
    "        yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}\n",
    "\n",
    "def input_preprocess(id2seq_file, ds_file, use_emb):\n",
    "    id2index = {}\n",
    "    seqs = []\n",
    "    index = 0\n",
    "    sid1_index = 0\n",
    "    sid2_index = 1\n",
    "    label_index = 2\n",
    "    \n",
    "    for line in open(id2seq_file):\n",
    "        line = line.strip().split('\\t')\n",
    "        id2index[line[0]] = index\n",
    "        seqs.append(line[1])\n",
    "        index += 1\n",
    "\n",
    "    seq_array = []\n",
    "    id2_aid = {}\n",
    "    sid = 0\n",
    "\n",
    "    seq2t = s2t(use_emb)\n",
    "    max_data = -1\n",
    "    limit_data = max_data > 0\n",
    "    raw_data = []\n",
    "    skip_head = True\n",
    "    x = None\n",
    "    count = 0\n",
    "    \n",
    "    # Create sequence array as a list of protein strings\n",
    "    for line in tqdm(open(ds_file)):\n",
    "        if skip_head:\n",
    "            skip_head = False\n",
    "            continue\n",
    "        line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
    "        if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
    "            continue\n",
    "        if id2_aid.get(line[sid1_index]) is None:\n",
    "            id2_aid[line[sid1_index]] = sid\n",
    "            sid += 1\n",
    "            seq_array.append(seqs[id2index[line[sid1_index]]])\n",
    "        line[sid1_index] = id2_aid[line[sid1_index]]\n",
    "        if id2_aid.get(line[sid2_index]) is None:\n",
    "            id2_aid[line[sid2_index]] = sid\n",
    "            sid += 1\n",
    "            seq_array.append(seqs[id2index[line[sid2_index]]])\n",
    "        line[sid2_index] = id2_aid[line[sid2_index]]\n",
    "        raw_data.append(line)\n",
    "        if limit_data:\n",
    "            count += 1\n",
    "            if count >= max_data:\n",
    "                break\n",
    "\n",
    "    len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
    "    avg_m_seq = int(np.average(len_m_seq)) + 1\n",
    "    max_m_seq = max(len_m_seq)\n",
    "    dim = seq2t.dim\n",
    "\n",
    "    # seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
    "    # Random for distribution of class labels\n",
    "    seq_tensor = np.array([seq2t.embed_normalized(line, seq_size) for line in tqdm(seq_array)]).astype('float16')\n",
    "\n",
    "    # Extract index of 1st and 2nd sequences in pairs\n",
    "    seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
    "    seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
    "\n",
    "    # Assign labels for pairs of sequences\n",
    "    class_map = {'0': 1, '1': 0}\n",
    "    class_labels = np.zeros((len(raw_data), 2))\n",
    "    for i in range(len(raw_data)):\n",
    "        class_labels[i][class_map[raw_data[i][label_index]]] = 1\n",
    "        \n",
    "    return seq_tensor, seq_index1, seq_index2, class_labels, dim\n",
    "\n",
    "def leaky_relu(x, alpha = .3):\n",
    "    return tf.keras.backend.maximum(alpha*x, x)\n",
    "\n",
    "def build_model(hparams):\n",
    "    # Input of sequence tensor representations \n",
    "    seq_input1 = Input(shape=(seq_size, dim), name='seq1')\n",
    "    seq_input2 = Input(shape=(seq_size, dim), name='seq2')\n",
    "\n",
    "    # Define Conv1D and Bi-RNN (GRU/LSTM) use in architecture\n",
    "    l1=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
    "    r1=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
    "    l2=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
    "    r2=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
    "    l3=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
    "    r3=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
    "    l4=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
    "    r4=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
    "    l5=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
    "    r5=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
    "    l6=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
    "    \n",
    "    # Siamese architecture\n",
    "\n",
    "    ### 1st sibling\n",
    "\n",
    "    # 1st Block RCNN \n",
    "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l1(seq_input1))\n",
    "    s1=concatenate([r1(s1), s1])\n",
    "\n",
    "    # 2nd Block RCNN\n",
    "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l2(s1))\n",
    "    s1=concatenate([r2(s1), s1])\n",
    "\n",
    "    # 3rd Block RCNN\n",
    "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l3(s1))\n",
    "    s1=concatenate([r3(s1), s1])\n",
    "\n",
    "    # 4th Block RCNN \n",
    "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l4(s1))\n",
    "    s1=concatenate([r4(s1), s1])\n",
    "\n",
    "    # 5th Block RCNN\n",
    "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l5(s1))\n",
    "    s1=concatenate([r5(s1), s1])\n",
    "    \n",
    "    # Last convolution\n",
    "    s1=l6(s1)\n",
    "    s1=GlobalAveragePooling1D()(s1)\n",
    "\n",
    "    ### 2nd sibling\n",
    "\n",
    "    # 1st block RCNN\n",
    "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l1(seq_input2))\n",
    "    s2=concatenate([r1(s2), s2])\n",
    "\n",
    "    # 2nd block RCNN\n",
    "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l2(s2))\n",
    "    s2=concatenate([r2(s2), s2])\n",
    "\n",
    "    # 3rd block RCNN\n",
    "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l3(s2))\n",
    "    s2=concatenate([r3(s2), s2])\n",
    "\n",
    "    # 4th block RCNN\n",
    "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l4(s2))\n",
    "    s2=concatenate([r4(s2), s2])\n",
    "\n",
    "    # 5th block RCNN\n",
    "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l5(s2))\n",
    "    s2=concatenate([r5(s2), s2])\n",
    "\n",
    "    # Last convolution\n",
    "    s2=l6(s2)\n",
    "    s2=GlobalAveragePooling1D()(s2)\n",
    "\n",
    "    ### Combine two siblings of siamese architecture\n",
    "    merge_text = multiply([s1, s2])\n",
    "    \n",
    "\n",
    "    #### MLP Part\n",
    "    # Set initializer\n",
    "    \n",
    "    # First dense\n",
    "    x = Dense(hparams[HP_FIRST_DENSE], activation=hparams[HP_ACTIVATION])(merge_text)\n",
    "    # x = tf.keras.layers.LeakyReLU(alpha=.3)(x)\n",
    "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
    "\n",
    "    # Second dense\n",
    "    x = Dense(int((hparams[HP_CONV_HIDDEN_DIM]+7)/2), activation=hparams[HP_ACTIVATION])(x)\n",
    "    # x = tf.keras.layers.LeakyReLU(alpha=.3)(x)\n",
    "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
    "\n",
    "    # Last softmax\n",
    "    main_output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Combine to form functional model\n",
    "    merge_model = Model(inputs=[seq_input1, seq_input2], outputs=[main_output])\n",
    "    return merge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TGt47faMW5U6"
   },
   "outputs": [],
   "source": [
    "# Default hyperparameters\n",
    "CONV_HIDDEN_DIM = 50\n",
    "RNN_HIDDEN = 50\n",
    "N_EPOCHS = 50\n",
    "HIDDEN_DIM=50\n",
    "BATCH_SIZE = 32\n",
    "DTYPE='float16'\n",
    "LEARNING_RATE=.001\n",
    "EPSILON=1e-6\n",
    "adam = Adam(learning_rate=LEARNING_RATE, amsgrad=True, epsilon=EPSILON)\n",
    "MAX_DATASET_SIZE = 11187\n",
    "DATASET_SIZE = MAX_DATASET_SIZE\n",
    "KERNEL_SIZE = 3\n",
    "POOLING_KERNEL = 3\n",
    "seq_size=3000\n",
    "dim = 1024\n",
    "get_custom_objects().update({'leaky_relu': leaky_relu})\n",
    "\n",
    "# 1 for language model embedding\n",
    "flags_embedding = 0\n",
    "# 1 for loading from drive\n",
    "available_data = 0\n",
    "\n",
    "cross_validation = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/pan_proteins.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_569498/3590616635.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# input_preprocess(id2seq_file=\"../data/pan_proteins.tsv\", ds_file=\"../data/pan_train.tsv\", use_emb=\"../data/ac5_aph.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mseq_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_index1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_index2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2seq_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/pan_proteins.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/pan_train.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/ac5_aph.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_569498/2192302697.py\u001b[0m in \u001b[0;36minput_preprocess\u001b[0;34m(id2seq_file, ds_file, use_emb)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlabel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2seq_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mid2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/pan_proteins.tsv'"
     ]
    }
   ],
   "source": [
    "# input_preprocess(id2seq_file=\"../data/pan_proteins.tsv\", ds_file=\"../data/pan_train.tsv\", use_emb=\"../data/ac5_aph.txt\")\n",
    "# train\n",
    "seq_tensor, seq_index1, seq_index2, class_labels = input_preprocess(id2seq_file=\"../data/pan_proteins.tsv\", ds_file=\"../data/pan_train.tsv\", use_emb=\"../data/ac5_aph.txt\")\n",
    "\n",
    "# test\n",
    "seq_tensor_, seq_index1_, seq_index2_, class_labels_ = input_preprocess(id2seq_file=\"../data/pan_proteins.tsv\", ds_file=\"../data/pan_test.tsv\", use_emb=\"../data/ac5_aph.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcEhSLxsONax"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter set by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default hyperparameters\n",
    "CONV_HIDDEN_DIM = 50\n",
    "RNN_HIDDEN = 50\n",
    "N_EPOCHS = 50\n",
    "HIDDEN_DIM=50\n",
    "BATCH_SIZE = 32\n",
    "DTYPE='float16'\n",
    "LEARNING_RATE=.001\n",
    "EPSILON=1e-6\n",
    "adam = Adam(learning_rate=LEARNING_RATE, amsgrad=True, epsilon=EPSILON)\n",
    "MAX_DATASET_SIZE = 11187\n",
    "DATASET_SIZE = MAX_DATASET_SIZE\n",
    "KERNEL_SIZE = 3\n",
    "POOLING_KERNEL = 3\n",
    "seq_size=2000\n",
    "dim = 1024\n",
    "# 1 for language model embedding\n",
    "flags_embedding = 0\n",
    "# 1 for loading from drive\n",
    "available_data = 0\n",
    "\n",
    "cross_validation = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KgZZ2JvoBLw",
    "tags": []
   },
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108156it [00:00, 564540.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8992/8992 [00:01<00:00, 4846.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 108155/108155 [00:00<00:00, 5751679.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 108155/108155 [00:00<00:00, 5785348.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# D-SCRIPT DATASET\n",
    "# seq_tensor, seq_index1, seq_index2, class_labels, dim = input_preprocess(id2seq_file='../data/wmbio_set/Train_set/human_pipr_deeptrio.fasta',\n",
    "#                                                                          ds_file='../data/wmbio_set/Train_set/human_train.tsv', \n",
    "#                                                                          use_emb = '../data/ac5_aph.txt')\n",
    "\n",
    "# seq_tensor_, seq_index1_, seq_index2_, class_labels_, dim = input_preprocess(id2seq_file='../data/wmbio_set/Train_set/human_pipr_deeptrio.fasta', \n",
    "#                                                                              ds_file='../data/wmbio_set/Train_set/human_test.tsv', \n",
    "#                                                                              use_emb = '../data/ac5_aph.txt')\n",
    "\n",
    "# EPIPR DATASET\n",
    "# seq_tensor, seq_index1, seq_index2, class_labels, dim = input_preprocess(id2seq_file='../data/wmbio_set/Train_set/human_custom_seq.tsv',\n",
    "#                                                                          ds_file='../data/wmbio_set/Train_set/human_custom_ppi_pair.tsv, \n",
    "#                                                                          use_emb = '../data/ac5_aph.txt')\n",
    "\n",
    "# seq_tensor_, seq_index1_, seq_index2_, class_labels_, dim = input_preprocess(id2seq_file='../data/pan_proteins.tsv', \n",
    "#                                                                              ds_file='../data/pan_test.tsv', \n",
    "#                                                                             use_emb = '../data/ac5_aph.txt')\n",
    "\n",
    "\n",
    "seq_tensor, seq_index1, seq_index2, class_labels, dim = input_preprocess(id2seq_file='data/wmbio_set/Train_set/human_custom_seq.tsv',\n",
    "                                                                         ds_file='data/wmbio_set/Train_set/human_custom_ppi_pair.tsv', \n",
    "                                                                         use_emb = 'data/ac5_aph.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HyK_KJ6LUfR"
   },
   "source": [
    "### Search for optimal configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SlRW8Nh71zjs"
   },
   "outputs": [],
   "source": [
    "HP_EPSILON = hp.HParam('epsilon', hp.Discrete([1e-6]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-3]))\n",
    "HP_FIRST_DENSE = hp.HParam('first_dense', hp.Discrete([100]))\n",
    "HP_KERNEL_SIZE = hp.HParam('kernel_size', hp.Discrete([3]))\n",
    "HP_POOLING_KERNEL = hp.HParam('pooling_kernel', hp.Discrete([3]))\n",
    "HP_CONV_HIDDEN_DIM = hp.HParam('conv_hidden_dim', hp.Discrete([50]))\n",
    "HP_RNN_HIDDEN_DIM = hp.HParam('rnn_hidden_dim', hp.Discrete([50]))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['leaky_relu']))\n",
    "HP_ACTIVATION_CONV = hp.HParam('activation_conv', hp.Discrete(['linear']))\n",
    "HP_REGULARIZER = hp.HParam('regularizer', hp.Discrete([0]))\n",
    "HP_CONV_PADDING = hp.HParam('conv_padding', hp.Discrete(['valid']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0e-1]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([256]))\n",
    "HP_LEAKY_RELU = hp.HParam('leaky_relu', hp.Discrete([3e-1]))\n",
    "METRIC_ACCURACY = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G34oBACWLqbw"
   },
   "source": [
    "### Define callbacks for monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VQdf1RAsYVcA"
   },
   "outputs": [],
   "source": [
    "### Learning rate schedule for optimization during training\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.4,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_lr=1e-5)\n",
    "\n",
    "# Schedule early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=6,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLqkyCvrruF-"
   },
   "source": [
    "### Define performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Um2gRMVrZPi",
    "outputId": "96eda6e8-15af-44a5-caa7-c2adea7d4fbf"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      # tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2, name='mcc'),\n",
    "      tfa.metrics.F1Score(num_classes=2, threshold=0.5, name='f1-score'),\n",
    "      # keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkG4si5QSf15"
   },
   "source": [
    "### Summary of model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0wMXJo9lqd6l",
    "outputId": "5ff95490-9d89-4b87-cc45-70b24481e945",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "  HP_EPSILON: EPSILON,\n",
    "  HP_LEARNING_RATE: LEARNING_RATE,\n",
    "  HP_FIRST_DENSE: 100,\n",
    "  HP_KERNEL_SIZE: 3,\n",
    "  HP_POOLING_KERNEL: 3,\n",
    "  HP_CONV_HIDDEN_DIM: 50,\n",
    "  HP_RNN_HIDDEN_DIM: 50,\n",
    "  HP_ACTIVATION: 'leaky_relu',\n",
    "  HP_ACTIVATION_CONV: 'relu',\n",
    "  HP_REGULARIZER: 0,\n",
    "  HP_CONV_PADDING: 'valid',\n",
    "  HP_DROPOUT: 3e-1,\n",
    "  HP_BATCH_SIZE: 256,\n",
    "  HP_LEAKY_RELU: 3e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Loop over all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b9czlGzdOR4",
    "outputId": "3c95c5fd-679e-42cb-abef-10769257e1bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge_model = None\n",
    "# merge_model = build_model(hparams)  \n",
    "\n",
    "\n",
    "# with tf.summary.create_file_writer(SAVE_MODEL + 'logs/hparam_tuning').as_default():\n",
    "#     hp.hparams_config(\n",
    "#     hparams=[HP_EPSILON,HP_LEARNING_RATE,HP_FIRST_DENSE, HP_KERNEL_SIZE, HP_POOLING_KERNEL, HP_CONV_HIDDEN_DIM, HP_RNN_HIDDEN_DIM, HP_ACTIVATION, HP_ACTIVATION_CONV, HP_REGULARIZER, HP_CONV_PADDING, HP_DROPOUT, HP_BATCH_SIZE, HP_LEAKY_RELU],\n",
    "#     metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "#   )\n",
    "\n",
    "\n",
    "# # ADAM\n",
    "# merge_model.compile(optimizer=Adam(learning_rate=hparams[HP_LEARNING_RATE], \n",
    "#                                    amsgrad=True, epsilon=hparams[HP_EPSILON]), \n",
    "#                     loss='categorical_crossentropy', metrics=METRICS)\n",
    "\n",
    "# # # SGD\n",
    "# # merge_model.compile(optimizer=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "# #                     loss='categorical_crossentropy', metrics=METRICS)\n",
    "\n",
    "\n",
    "# # Create train\n",
    "# train_dataset = tf.data.Dataset.from_generator(generator_pair, \n",
    "#                                                args=[seq_tensor, class_labels, np.arange(len(class_labels))], \n",
    "#                                                output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), \n",
    "#                                                output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
    "# train_dataset = train_dataset.shuffle(1024).repeat(N_EPOCHS).batch(hparams[HP_BATCH_SIZE])\n",
    "# train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # # Create test\n",
    "# test_dataset = tf.data.Dataset.from_generator(generator_pair, args=[seq_tensor_, class_labels_, np.arange(len(class_labels_))], \n",
    "#                                               output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), \n",
    "#                                               output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
    "# test_dataset = test_dataset.batch(hparams[HP_BATCH_SIZE])\n",
    "\n",
    "\n",
    "# # Save the best model base on val_accuracy\n",
    "# checkpoint = ModelCheckpoint(filepath='pipr_best_model.hdf5', monitor='val_loss',verbose=1, \n",
    "#                              save_best_only=True, mode='min')\n",
    "\n",
    "# # Fit model\n",
    "# merge_model.fit(train_dataset, \n",
    "#                 steps_per_epoch=len(seq_tensor) // 128, \n",
    "#                 epochs=N_EPOCHS, \n",
    "#                 validation_data=test_dataset,\n",
    "#                 # validation_split = 0.2,\n",
    "#                 callbacks=[checkpoint, reduce_lr, early_stopping])\n",
    "\n",
    "\n",
    "# # merge_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Training time =====================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 09:00:31.716999: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-08 09:00:34.528182: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n",
      "2022-03-08 09:00:35.906688: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 249s 339ms/step - loss: 0.4507 - accuracy: 0.7717 - precision: 0.7717 - recall: 0.7717 - f1-score: 0.7370 - prc: 0.8671 - val_loss: 0.1655 - val_accuracy: 0.9376 - val_precision: 0.9376 - val_recall: 0.9376 - val_f1-score: 0.9348 - val_prc: 0.9808\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.93764, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmbio/anaconda3/envs/pipr/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 226s 335ms/step - loss: 0.1308 - accuracy: 0.9557 - precision: 0.9557 - recall: 0.9557 - f1-score: 0.9532 - prc: 0.9869 - val_loss: 0.1523 - val_accuracy: 0.9498 - val_precision: 0.9498 - val_recall: 0.9498 - val_f1-score: 0.9459 - val_prc: 0.9830\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.93764 to 0.94979, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 3/50\n",
      "675/675 [==============================] - 225s 334ms/step - loss: 0.0964 - accuracy: 0.9673 - precision: 0.9673 - recall: 0.9673 - f1-score: 0.9654 - prc: 0.9921 - val_loss: 0.0950 - val_accuracy: 0.9687 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1-score: 0.9670 - val_prc: 0.9914\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.94979 to 0.96870, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 4/50\n",
      "675/675 [==============================] - 225s 333ms/step - loss: 0.0785 - accuracy: 0.9740 - precision: 0.9740 - recall: 0.9740 - f1-score: 0.9725 - prc: 0.9944 - val_loss: 0.0889 - val_accuracy: 0.9729 - val_precision: 0.9729 - val_recall: 0.9729 - val_f1-score: 0.9714 - val_prc: 0.9915\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.96870 to 0.97286, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 5/50\n",
      "675/675 [==============================] - 227s 337ms/step - loss: 0.0664 - accuracy: 0.9782 - precision: 0.9782 - recall: 0.9782 - f1-score: 0.9770 - prc: 0.9956 - val_loss: 0.0951 - val_accuracy: 0.9714 - val_precision: 0.9714 - val_recall: 0.9714 - val_f1-score: 0.9697 - val_prc: 0.9910\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.97286\n",
      "Epoch 6/50\n",
      "675/675 [==============================] - 228s 338ms/step - loss: 0.0590 - accuracy: 0.9806 - precision: 0.9806 - recall: 0.9806 - f1-score: 0.9795 - prc: 0.9965 - val_loss: 0.0920 - val_accuracy: 0.9731 - val_precision: 0.9731 - val_recall: 0.9731 - val_f1-score: 0.9715 - val_prc: 0.9915\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.97286 to 0.97314, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 7/50\n",
      "675/675 [==============================] - 224s 332ms/step - loss: 0.0511 - accuracy: 0.9831 - precision: 0.9831 - recall: 0.9831 - f1-score: 0.9821 - prc: 0.9972 - val_loss: 0.1125 - val_accuracy: 0.9680 - val_precision: 0.9680 - val_recall: 0.9680 - val_f1-score: 0.9658 - val_prc: 0.9883\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.97314\n",
      "Epoch 8/50\n",
      "675/675 [==============================] - 229s 340ms/step - loss: 0.0455 - accuracy: 0.9849 - precision: 0.9849 - recall: 0.9849 - f1-score: 0.9840 - prc: 0.9977 - val_loss: 0.0954 - val_accuracy: 0.9754 - val_precision: 0.9754 - val_recall: 0.9754 - val_f1-score: 0.9740 - val_prc: 0.9909\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.97314 to 0.97541, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 9/50\n",
      "675/675 [==============================] - 226s 335ms/step - loss: 0.0412 - accuracy: 0.9861 - precision: 0.9861 - recall: 0.9861 - f1-score: 0.9853 - prc: 0.9981 - val_loss: 0.0970 - val_accuracy: 0.9756 - val_precision: 0.9756 - val_recall: 0.9756 - val_f1-score: 0.9741 - val_prc: 0.9907\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.97541 to 0.97564, saving model to save_model/1-fold_best_model.hdf5\n",
      "Epoch 10/50\n",
      "675/675 [==============================] - 224s 331ms/step - loss: 0.0364 - accuracy: 0.9877 - precision: 0.9877 - recall: 0.9877 - f1-score: 0.9869 - prc: 0.9984 - val_loss: 0.0901 - val_accuracy: 0.9780 - val_precision: 0.9780 - val_recall: 0.9780 - val_f1-score: 0.9767 - val_prc: 0.9920\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.97564 to 0.97804, saving model to save_model/1-fold_best_model.hdf5\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "==================== Training time =====================\n",
      "Epoch 1/50\n",
      "675/675 [==============================] - 237s 338ms/step - loss: 0.4919 - accuracy: 0.7745 - precision: 0.7745 - recall: 0.7745 - f1-score: 0.7401 - prc: 0.8711 - val_loss: 0.1699 - val_accuracy: 0.9380 - val_precision: 0.9380 - val_recall: 0.9380 - val_f1-score: 0.9343 - val_prc: 0.9793\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.93801, saving model to save_model/2-fold_best_model.hdf5\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmbio/anaconda3/envs/pipr/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 227s 336ms/step - loss: 0.1350 - accuracy: 0.9544 - precision: 0.9544 - recall: 0.9544 - f1-score: 0.9519 - prc: 0.9861 - val_loss: 0.1085 - val_accuracy: 0.9634 - val_precision: 0.9634 - val_recall: 0.9634 - val_f1-score: 0.9610 - val_prc: 0.9900\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.93801 to 0.96339, saving model to save_model/2-fold_best_model.hdf5\n",
      "Epoch 3/50\n",
      "675/675 [==============================] - 225s 334ms/step - loss: 0.0990 - accuracy: 0.9666 - precision: 0.9666 - recall: 0.9666 - f1-score: 0.9647 - prc: 0.9918 - val_loss: 0.1267 - val_accuracy: 0.9573 - val_precision: 0.9573 - val_recall: 0.9573 - val_f1-score: 0.9539 - val_prc: 0.9880\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.96339\n",
      "Epoch 4/50\n",
      "675/675 [==============================] - 228s 338ms/step - loss: 0.0794 - accuracy: 0.9739 - precision: 0.9739 - recall: 0.9739 - f1-score: 0.9724 - prc: 0.9942 - val_loss: 0.1058 - val_accuracy: 0.9638 - val_precision: 0.9638 - val_recall: 0.9638 - val_f1-score: 0.9610 - val_prc: 0.9908\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.96339 to 0.96380, saving model to save_model/2-fold_best_model.hdf5\n",
      "Epoch 5/50\n",
      "675/675 [==============================] - 233s 345ms/step - loss: 0.0684 - accuracy: 0.9781 - precision: 0.9781 - recall: 0.9781 - f1-score: 0.9769 - prc: 0.9954 - val_loss: 0.0848 - val_accuracy: 0.9735 - val_precision: 0.9735 - val_recall: 0.9735 - val_f1-score: 0.9717 - val_prc: 0.9927\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.96380 to 0.97351, saving model to save_model/2-fold_best_model.hdf5\n",
      "Epoch 6/50\n",
      "675/675 [==============================] - 229s 340ms/step - loss: 0.0582 - accuracy: 0.9813 - precision: 0.9813 - recall: 0.9813 - f1-score: 0.9803 - prc: 0.9964 - val_loss: 0.0779 - val_accuracy: 0.9747 - val_precision: 0.9747 - val_recall: 0.9747 - val_f1-score: 0.9730 - val_prc: 0.9943\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.97351 to 0.97471, saving model to save_model/2-fold_best_model.hdf5\n",
      "Epoch 7/50\n",
      "675/675 [==============================] - 231s 342ms/step - loss: 0.0545 - accuracy: 0.9826 - precision: 0.9826 - recall: 0.9826 - f1-score: 0.9816 - prc: 0.9969 - val_loss: 0.0750 - val_accuracy: 0.9755 - val_precision: 0.9755 - val_recall: 0.9755 - val_f1-score: 0.9738 - val_prc: 0.9943\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.97471 to 0.97550, saving model to save_model/2-fold_best_model.hdf5\n",
      "Epoch 8/50\n",
      " 35/675 [>.............................] - ETA: 3:12 - loss: 0.0508 - accuracy: 0.9828 - precision: 0.9828 - recall: 0.9828 - f1-score: 0.9819 - prc: 0.9978"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=5, test_size=2, random_state=331)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cnt = 0\n",
    "\n",
    "# save models\n",
    "SAVE_MODEL = 'save_model/'\n",
    "Path(SAVE_MODEL).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# log\n",
    "with tf.summary.create_file_writer(SAVE_MODEL + 'logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_EPSILON,HP_LEARNING_RATE,HP_FIRST_DENSE, HP_KERNEL_SIZE, HP_POOLING_KERNEL, HP_CONV_HIDDEN_DIM, HP_RNN_HIDDEN_DIM, HP_ACTIVATION, HP_ACTIVATION_CONV, HP_REGULARIZER, HP_CONV_PADDING, HP_DROPOUT, HP_BATCH_SIZE, HP_LEAKY_RELU],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "\n",
    "for train, test in kf.split(class_labels):\n",
    "    cnt+=1\n",
    "    merge_model = None\n",
    "    merge_model = build_model(hparams)  \n",
    "    tf.keras.utils.plot_model(merge_model, to_file=SAVE_MODEL + 'model.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    merge_model.compile(optimizer=Adam(learning_rate=hparams[HP_LEARNING_RATE], amsgrad=True, epsilon=hparams[HP_EPSILON]), \n",
    "                      loss='categorical_crossentropy', metrics=METRICS)\n",
    "    \n",
    "    # Create train\n",
    "    # from generator\n",
    "    train_dataset = tf.data.Dataset.from_generator(generator_pair, \n",
    "                                                   args=[seq_tensor, class_labels, train], \n",
    "                                                   output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), \n",
    "                                                   output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
    "    train_dataset = train_dataset.shuffle(1024).repeat(N_EPOCHS).batch(hparams[HP_BATCH_SIZE])\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create test\n",
    "    test_dataset = tf.data.Dataset.from_generator(generator_pair, args=[seq_tensor, class_labels, test], \n",
    "                                                  output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), \n",
    "                                                  output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
    "    test_dataset = test_dataset.batch(hparams[HP_BATCH_SIZE])\n",
    "    \n",
    "    # Save the best model base on val_accuracy\n",
    "    checkpoint = ModelCheckpoint(filepath=SAVE_MODEL + str(cnt)+'-fold_best_model.hdf5', \n",
    "                                 monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Fit model\n",
    "    print(f'==================== Training time =====================')\n",
    "    history_model = merge_model.fit(train_dataset, \n",
    "                                    epochs=N_EPOCHS, \n",
    "                                    steps_per_epoch=len(train) // 128, \n",
    "                                    validation_data=test_dataset,\n",
    "                                    callbacks=[checkpoint, reduce_lr, early_stopping,                                               \n",
    "                                              tf.keras.callbacks.CSVLogger(SAVE_MODEL + 'history.csv')])\n",
    "    # file rename\n",
    "    shutil.move(SAVE_MODEL + 'history.csv', SAVE_MODEL + str(cnt) + '-fold_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Evaluation-Benchmark-dataset-baseline-PIPR.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pipr",
   "language": "python",
   "name": "pipr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
